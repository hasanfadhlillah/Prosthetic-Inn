{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["### Tahap 1: Import Libraries"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install tensorflow"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from scipy.signal import filtfilt, butter, iirnotch\n", "from sklearn.model_selection import train_test_split, KFold\n", "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc, classification_report\n", "import tensorflow as tf\n", "from tensorflow.keras.models import Model\n", "from tensorflow.keras.layers import Input, Conv3D, MaxPooling3D, Flatten, Dense, concatenate, Dropout\n", "from tensorflow.keras.utils import to_categorical\n", "from sklearn.model_selection import train_test_split, StratifiedKFold\n", "from tensorflow.keras.optimizers import Adam\n", "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc\n", "from sklearn.preprocessing import label_binarize, LabelEncoder, OneHotEncoder\n", "\n", "import warnings\n", "warnings.filterwarnings(\"ignore\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Tahap 2: Memuat Dataset dan Informasi Awal"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Memuat dataset\n", "data = pd.read_csv(\"../Datasets/EMG-data.csv\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Melihat data\n", "data.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Melihat informasi data\n", "data.info()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Melihat statistik deskriptif data\n", "data.describe()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Memisahkan dataset berdasarkan kelas hand gesture\n", "gesture_datasets = {}\n", "for gesture_class in range(8):\n", "    gesture_datasets[gesture_class] = data[data['class'] == gesture_class]\n", "\n", "# Menampilkan jumlah data pada masing-masing kelas\n", "for gesture_class in gesture_datasets:\n", "    print(f\"Kelas {gesture_class} memiliki {gesture_datasets[gesture_class].shape[0]} data\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Tahap 3: Filtering"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Exponential Filtering"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Exponential filter function for 1D array\n", "def exponential_filter(data, alpha=0.5):\n", "    filtered_data = np.zeros(data.shape)\n", "    if len(data) > 0:\n", "        filtered_data[0] = data[0]\n", "        for t in range(1, len(data)):\n", "            filtered_data[t] = alpha * data[t] + (1 - alpha) * filtered_data[t-1]\n", "    return filtered_data\n", "\n", "# Apply exponential filter to each EMG channel in the dataframe\n", "def apply_exponential_filter_to_df(df, emg_channels, alpha=0.5):\n", "    filtered_df = df.copy()\n", "    for channel in emg_channels:\n", "        filtered_df[channel] = exponential_filter(df[channel].values, alpha)\n", "    return filtered_df\n", "\n", "# Define EMG channels\n", "emg_channels = data.columns[1:9]  # Columns for channels 1-8\n", "\n", "# Apply exponential filter to each subset of the dataset\n", "filtered_gesture_datasets = {}\n", "for gesture_class in gesture_datasets:\n", "    filtered_gesture_datasets[gesture_class] = apply_exponential_filter_to_df(gesture_datasets[gesture_class], emg_channels, alpha=0.5)\n", "\n", "# Combine filtered data back into one dataset\n", "filtered_data = pd.concat(filtered_gesture_datasets.values(), ignore_index=True)\n", "\n", "# Display first few rows of filtered data\n", "print(filtered_data.head())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Plotting function\n", "def plot_filtered_data(subject_data, filtered_data, channel_names):\n", "    for gesture_class in range(8):\n", "        original_gesture_data = subject_data[subject_data['class'] == gesture_class]\n", "        filtered_gesture_data = filtered_data[filtered_data['class'] == gesture_class]\n", "\n", "        for channel_name in channel_names:\n", "            plt.figure(figsize=(15, 5))\n", "\n", "            original_data = original_gesture_data[channel_name].values\n", "            filtered_channel_exp = filtered_gesture_data[channel_name].values\n", "\n", "            plt.plot(original_data, label='Original {}'.format(channel_name))\n", "            plt.plot(filtered_channel_exp, label='Exponential Filtered {}'.format(channel_name))\n", "\n", "            plt.xlabel('Waktu (ms)')\n", "            plt.ylabel('Amplitudo')\n", "            plt.title('Hand Gesture Class {} - {} Filter'.format(gesture_class, channel_name))\n", "            plt.legend()\n", "\n", "            plt.tight_layout()\n", "            plt.show()\n", "\n", "# Get data for subject 11\n", "subject_data = data[data['label'] == 11]\n", "filtered_subject_data = filtered_data[filtered_data['label'] == 11]\n", "channel_names = ['channel1', 'channel2', 'channel3', 'channel4', 'channel5', 'channel6', 'channel7', 'channel8']\n", "\n", "# Plot data for subject 11\n", "plot_filtered_data(subject_data, filtered_subject_data, channel_names)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Notch Filtering"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Calculate the sampling frequency (fs) for each subset\n", "def calculate_fs(data):\n", "    time_diff = data['time'].diff().dropna().mean()\n", "    fs = 1000 / time_diff  # Assuming time is in milliseconds\n", "    return fs\n", "\n", "# Calculate fs for each gesture subset\n", "fs_gesture_datasets = {gesture_class: calculate_fs(gesture_datasets[gesture_class]) for gesture_class in gesture_datasets}\n", "\n", "# Print sampling frequencies\n", "for gesture_class, fs in fs_gesture_datasets.items():\n", "    print(f\"Frekuensi sampling untuk kelas {gesture_class}: {fs:.2f} Hz\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Notch filter function\n", "def notch_filter(data, fs, freq=50.0, Q=30.0):\n", "    b, a = iirnotch(freq, Q, fs)\n", "    filtered_data = filtfilt(b, a, data)\n", "    return filtered_data\n", "\n", "# Apply notch filter to each subset of the dataset\n", "def apply_notch_filter_to_df(df, emg_channels, fs, freq=50.0, Q=30.0):\n", "    filtered_df = df.copy()\n", "    for channel in emg_channels:\n", "        filtered_df[channel] = notch_filter(df[channel].values, fs, freq, Q)\n", "    return filtered_df\n", "\n", "# Apply notch filter to each subset of the dataset using their respective fs\n", "notch_filtered_gesture_datasets = {gesture_class: apply_notch_filter_to_df(filtered_gesture_datasets[gesture_class], emg_channels, fs_gesture_datasets[gesture_class]) for gesture_class in filtered_gesture_datasets}\n", "\n", "# Combine notch filtered data back into one dataset\n", "notch_filtered_data = pd.concat(notch_filtered_gesture_datasets.values(), ignore_index=True)\n", "\n", "# Display first few rows of notch filtered data\n", "print(notch_filtered_data.head())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Plotting function for notch filter comparison\n", "def plot_notch_filtered_data(subject_data, filtered_data_exp, filtered_data_notch, channel_names):\n", "    for gesture_class in range(8):\n", "        original_gesture_data = subject_data[subject_data['class'] == gesture_class]\n", "        exp_filtered_gesture_data = filtered_data_exp[filtered_data_exp['class'] == gesture_class]\n", "        notch_filtered_gesture_data = filtered_data_notch[filtered_data_notch['class'] == gesture_class]\n", "\n", "        for channel_name in channel_names:\n", "            plt.figure(figsize=(15, 5))\n", "\n", "            original_data = original_gesture_data[channel_name].values\n", "            exp_filtered_data = exp_filtered_gesture_data[channel_name].values\n", "            notch_filtered_data = notch_filtered_gesture_data[channel_name].values\n", "\n", "            plt.plot(original_data, label='Original {}'.format(channel_name))\n", "            plt.plot(exp_filtered_data, label='Exponential Filtered {}'.format(channel_name))\n", "            plt.plot(notch_filtered_data, label='Notch Filtered {}'.format(channel_name))\n", "\n", "            plt.xlabel('Waktu (ms)')\n", "            plt.ylabel('Amplitudo')\n", "            plt.title('Hand Gesture Class {} - {} Filter'.format(gesture_class, channel_name))\n", "            plt.legend()\n", "\n", "            plt.tight_layout()\n", "            plt.show()\n", "\n", "# Get data for subject 11\n", "subject_data = data[data['label'] == 11]\n", "exp_filtered_subject_data = filtered_data[filtered_data['label'] == 11]\n", "notch_filtered_subject_data = notch_filtered_data[notch_filtered_data['label'] == 11]\n", "channel_names = ['channel1', 'channel2', 'channel3', 'channel4', 'channel5', 'channel6', 'channel7', 'channel8']\n", "\n", "# Plot data for subject 11\n", "plot_notch_filtered_data(subject_data, exp_filtered_subject_data, notch_filtered_subject_data, channel_names)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Tahap 4: Segmentation"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Adjacent Windowing"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Define window duration in seconds and overlap fraction\n", "window_duration = 3  # 3 second\n", "overlap_fraction = 0.5  # 50% overlap\n", "\n", "# Function to create segments with a consistent window size\n", "def create_segments(data, window_size, overlap):\n", "    segments = []\n", "    labels = []\n", "    num_samples = len(data)\n", "    step = window_size - overlap\n", "    for start in range(0, num_samples - window_size + 1, step):\n", "        segment = data.iloc[start:start+window_size, 1:9].values  # Take columns channel1 to channel8\n", "        label = data.iloc[start:start+window_size]['label'].mode()[0]  # Take the most frequent label in the window\n", "        segments.append(segment)\n", "        labels.append(label)\n", "    return np.array(segments), np.array(labels)\n", "\n", "# Calculate the lowest sampling frequency\n", "min_fs = min(fs_gesture_datasets.values())\n", "\n", "# Calculate consistent window size and overlap using the lowest sampling frequency\n", "window_size = int(window_duration * min_fs)\n", "overlap = int(window_size * overlap_fraction)\n", "\n", "# Create segments for each gesture class using consistent window size\n", "gesture_segments = {}\n", "gesture_labels = {}\n", "\n", "for gesture_class in gesture_datasets:\n", "    subset_data = gesture_datasets[gesture_class]\n", "    segments, labels = create_segments(subset_data, window_size, overlap)\n", "    gesture_segments[gesture_class] = segments\n", "    gesture_labels[gesture_class] = labels\n", "\n", "# Combine segments and labels for all gesture classes\n", "all_segments = np.concatenate([gesture_segments[gc] for gc in gesture_segments if gesture_segments[gc].size > 0], axis=0)\n", "all_labels = np.concatenate([gesture_labels[gc] for gc in gesture_labels if gesture_labels[gc].size > 0], axis=0)\n", "\n", "print(f\"Total segments: {all_segments.shape[0]}\")\n", "print(f\"Total labels: {all_labels.shape[0]}\")\n", "\n", "# Ensure the dimensions of segments are consistent\n", "segment_lengths = [segments.shape[1] for segments in gesture_segments.values() if segments.size > 0]\n", "print(f\"Segment lengths: {segment_lengths}\")\n", "\n", "# Check if all segment lengths are the same\n", "if len(set(segment_lengths)) != 1:\n", "    print(\"Error: Not all segment lengths are the same.\")\n", "else:\n", "    print(\"All segment lengths are consistent.\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "\n", "def plot_segment(segment, fs, channel_names):\n", "    time_axis = np.linspace(0, len(segment) / fs, len(segment))\n", "    plt.figure(figsize=(15, 5))\n", "    for i, channel in enumerate(channel_names):\n", "        plt.plot(time_axis, segment[:, i], label=channel)\n", "    plt.xlabel('Waktu (detik)')\n", "    plt.ylabel('Amplitudo')\n", "    plt.title('Segmentasi Data')\n", "    plt.legend()\n", "    plt.show()\n", "\n", "# Plot the first segment of each gesture class for subject 11\n", "subject_id = 11\n", "subject_segments = {gesture_class: gesture_segments[gesture_class][gesture_labels[gesture_class] == subject_id] for gesture_class in gesture_segments if gesture_segments[gesture_class].size > 0}\n", "channel_names = ['channel1', 'channel2', 'channel3', 'channel4', 'channel5', 'channel6', 'channel7', 'channel8']\n", "\n", "for gesture_class in subject_segments:\n", "    if len(subject_segments[gesture_class]) > 0:\n", "        plot_segment(subject_segments[gesture_class][0], fs_gesture_datasets[gesture_class], channel_names)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Tahap 5: Feature Extraction"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Function to calculate Mean Absolute Value (MAV)\n", "def calculate_mav(segment):\n", "    return np.mean(np.abs(segment), axis=0)\n", "\n", "# Function to calculate Root Mean Square (RMS)\n", "def calculate_rms(segment):\n", "    return np.sqrt(np.mean(segment**2, axis=0))\n", "\n", "# Function to calculate Amplitude (v)\n", "def calculate_amplitude(segment):\n", "    return np.max(segment, axis=0)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Initialize dictionaries to store feature values for each gesture class\n", "mav_features = {}\n", "rms_features = {}\n", "amplitude_features = {}\n", "\n", "# Extract features for each gesture class\n", "for gesture_class in gesture_segments:\n", "    segments = gesture_segments[gesture_class]\n", "    \n", "    mav_values = np.array([calculate_mav(segment) for segment in segments])\n", "    rms_values = np.array([calculate_rms(segment) for segment in segments])\n", "    amplitude_values = np.array([calculate_amplitude(segment) for segment in segments])\n", "    \n", "    mav_features[gesture_class] = mav_values\n", "    rms_features[gesture_class] = rms_values\n", "    amplitude_features[gesture_class] = amplitude_values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Convert to DataFrame for better visualization\n", "mav_df = pd.DataFrame({f'class_{gc}': mav_features[gc].mean(axis=0) for gc in mav_features})\n", "rms_df = pd.DataFrame({f'class_{gc}': rms_features[gc].mean(axis=0) for gc in rms_features})\n", "amplitude_df = pd.DataFrame({f'class_{gc}': amplitude_features[gc].mean(axis=0) for gc in amplitude_features})"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Display feature values in tabular form\n", "print(\"Mean Absolute Value (MAV):\")\n", "print(mav_df)\n", "\n", "print(\"\\nRoot Mean Square (RMS):\")\n", "print(rms_df)\n", "\n", "print(\"\\nAmplitude (v):\")\n", "print(amplitude_df)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Calculate overall mean for each feature across all gesture classes\n", "overall_mav = mav_df.mean(axis=1)\n", "overall_rms = rms_df.mean(axis=1)\n", "overall_amplitude = amplitude_df.mean(axis=1)\n", "\n", "print(\"\\nOverall Mean Absolute Value (MAV) across all gesture classes:\")\n", "print(overall_mav)\n", "\n", "print(\"\\nOverall Root Mean Square (RMS) across all gesture classes:\")\n", "print(overall_rms)\n", "\n", "print(\"\\nOverall Amplitude (v) across all gesture classes:\")\n", "print(overall_amplitude)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Tahap 6: Modeling Multi-Stream 3D CNN"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Reshape untuk 3D CNN Model Input Layer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Constants\n", "num_classes = 8\n", "num_channels = 8  # Number of EMG channels\n", "\n", "# Reshape feature arrays for 3D CNN\n", "def reshape_features(features, num_channels):\n", "    num_samples = features.shape[1] // num_channels\n", "    reshaped = features.reshape((features.shape[0], num_samples, num_channels, 1, 1))\n", "    return reshaped\n", "\n", "# Print the shapes before reshaping\n", "print(\"Shapes before reshaping:\")\n", "print(f\"MAV shapes: {[mav_features[gc].shape for gc in mav_features]}\")\n", "print(f\"RMS shapes: {[rms_features[gc].shape for gc in rms_features]}\")\n", "print(f\"Amplitude shapes: {[amplitude_features[gc].shape for gc in amplitude_features]}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Reshape MAV, RMS, and Amplitude features\n", "mav_reshaped = {gc: reshape_features(mav_features[gc], num_channels) for gc in mav_features}\n", "rms_reshaped = {gc: reshape_features(rms_features[gc], num_channels) for gc in rms_features}\n", "amplitude_reshaped = {gc: reshape_features(amplitude_features[gc], num_channels) for gc in amplitude_features}\n", "\n", "# Combine all reshaped features\n", "X_mav = np.concatenate([mav_reshaped[gc] for gc in mav_reshaped if mav_reshaped[gc].size > 0], axis=0)\n", "X_rms = np.concatenate([rms_reshaped[gc] for gc in rms_reshaped if rms_reshaped[gc].size > 0], axis=0)\n", "X_amplitude = np.concatenate([amplitude_reshaped[gc] for gc in amplitude_reshaped if amplitude_reshaped[gc].size > 0], axis=0)\n", "\n", "# Labels\n", "y = np.concatenate([gesture_labels[gc] for gc in gesture_labels if gesture_labels[gc].size > 0], axis=0)\n", "\n", "# Check for any labels that are out of bounds\n", "if np.any(y >= num_classes):\n", "    print(f\"Warning: Found labels out of bounds: {y[y >= num_classes]}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Ensure all labels are within the correct range\n", "y = np.clip(y, 0, num_classes - 1)\n", "\n", "# Convert labels to categorical\n", "y = to_categorical(y, num_classes=num_classes)\n", "\n", "# Print shapes to confirm\n", "print(f'X_mav shape: {X_mav.shape}')\n", "print(f'X_rms shape: {X_rms.shape}')\n", "print(f'X_amplitude shape: {X_amplitude.shape}')\n", "print(f'y shape: {y.shape}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Multi-Stream 3D CNN"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Define 3D CNN for each stream\n", "def create_cnn(input_layer):\n", "    x = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(input_layer)\n", "    x = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(x)\n", "    x = MaxPooling3D((1, 1, 1))(x)\n", "    x = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(x)\n", "    x = MaxPooling3D((1, 1, 1))(x)\n", "    x = Flatten()(x)\n", "    return x\n", "\n", "# Input layers\n", "input_mav = Input(shape=(X_mav.shape[1], X_mav.shape[2], X_mav.shape[3], X_mav.shape[4]))\n", "input_rms = Input(shape=(X_rms.shape[1], X_rms.shape[2], X_rms.shape[3], X_rms.shape[4]))\n", "input_amplitude = Input(shape=(X_amplitude.shape[1], X_amplitude.shape[2], X_amplitude.shape[3], X_amplitude.shape[4]))\n", "\n", "# Create streams\n", "stream_mav = create_cnn(input_mav)\n", "stream_rms = create_cnn(input_rms)\n", "stream_amplitude = create_cnn(input_amplitude)\n", "\n", "# Combine streams\n", "combined = concatenate([stream_mav, stream_rms, stream_amplitude])\n", "\n", "# Fully connected layers\n", "x = Dense(512, activation='relu')(combined)\n", "x = Dense(128, activation='relu')(x)\n", "output = Dense(num_classes, activation='softmax')(x)\n", "\n", "# Define model\n", "model = Model(inputs=[input_mav, input_rms, input_amplitude], outputs=output)\n", "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Train the model\n", "history = model.fit([X_mav, X_rms, X_amplitude], y, epochs=50, batch_size=32, validation_split=0.2)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Tahap 7: Evaluation Model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Lakukan prediksi\n", "y_pred = model.predict([X_mav, X_rms, X_amplitude])\n", "\n", "# Buat prediksi untuk setiap kelas hand gesture\n", "y_pred_classes = np.argmax(y_pred, axis=1)\n", "y_test_classes = np.argmax(y, axis=1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Classification Report"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Classification Report untuk setiap hand gesture\n", "for i in range(num_classes):\n", "    y_true = (y_test_classes == i).astype(int)\n", "    y_pred_class = (y_pred_classes == i).astype(int)\n", "    print(f'Classification Report for Class {i}:\\n')\n", "    print(classification_report(y_true, y_pred_class))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Classification Report secara keseluruhan\n", "print('Overall Classification Report:\\n')\n", "print(classification_report(y_test_classes, y_pred_classes))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Accuracy"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Accuracy untuk setiap hand gesture\n", "for i in range(num_classes):\n", "    y_true = (y_test_classes == i).astype(int)\n", "    y_pred_class = (y_pred_classes == i).astype(int)\n", "    acc = accuracy_score(y_true, y_pred_class)\n", "    print(f'Accuracy for Class {i}: {acc:.2f}')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Accuracy secara keseluruhan\n", "overall_accuracy = accuracy_score(y_test_classes, y_pred_classes)\n", "print(f'Overall Accuracy: {overall_accuracy * 100:.2f}%')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize training & validation accuracy and loss\n", "plt.figure(figsize=(12, 5))\n", "\n", "# Accuracy\n", "plt.subplot(1, 2, 1)\n", "plt.plot(history.history['accuracy'], label='Training Accuracy')\n", "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n", "plt.title('Training and Validation Accuracy')\n", "plt.xlabel('Epochs')\n", "plt.ylabel('Accuracy')\n", "plt.legend()\n", "\n", "# Loss\n", "plt.subplot(1, 2, 2)\n", "plt.plot(history.history['loss'], label='Training Loss')\n", "plt.plot(history.history['val_loss'], label='Validation Loss')\n", "plt.title('Training and Validation Loss')\n", "plt.xlabel('Epochs')\n", "plt.ylabel('Loss')\n", "plt.legend()\n", "\n", "plt.tight_layout()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Confusion Matrix"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Confusion matrix\n", "conf_matrix = confusion_matrix(y_test_classes, y_pred_classes)\n", "plt.figure(figsize=(10, 8))\n", "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(num_classes), yticklabels=range(num_classes))\n", "plt.title('Confusion Matrix')\n", "plt.xlabel('Predicted Label')\n", "plt.ylabel('True Label')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### ROC curve"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ROC curve\n", "fpr = dict()\n", "tpr = dict()\n", "roc_auc = dict()\n", "for i in range(num_classes):\n", "    fpr[i], tpr[i], _ = roc_curve(y[:, i], y_pred[:, i])\n", "    roc_auc[i] = auc(fpr[i], tpr[i])\n", "\n", "plt.figure(figsize=(10, 8))\n", "for i in range(num_classes):\n", "    plt.plot(fpr[i], tpr[i], label=f'ROC curve (area = {roc_auc[i]:.2f}) for class {i}')\n", "plt.plot([0, 1], [0, 1], 'k--')\n", "plt.xlim([0.0, 1.0])\n", "plt.ylim([0.0, 1.05])\n", "plt.xlabel('False Positive Rate')\n", "plt.ylabel('True Positive Rate')\n", "plt.title('Receiver Operating Characteristic (ROC)')\n", "plt.legend(loc='lower right')\n", "plt.show()"]}], "metadata": {"kernelspec": {"display_name": "dsml", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.11"}}, "nbformat": 4, "nbformat_minor": 2}